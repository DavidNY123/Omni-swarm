{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:From <ipython-input-1-166c1ac4ae91>:40: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "tensorflow.keras.backend.set_learning_phase(0)\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.framework import convert_to_constants\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.image import resampler\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfa.image.resampler\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "import numpy as np\n",
    "model_dir = \"./config/hfnet_v2\"\n",
    "output_saved_model_dir = \"./config/hfnet_v2_trt\" \n",
    "output_frozen_path = \"./config/hf_frozen_208x400_v2.pb\"\n",
    "verify_images_path= \"/ssd/drone_data_process/images\"\n",
    "\n",
    "\n",
    "img = cv2.imread(\"./db1.jpg\")\n",
    "img = cv2.resize(img, (400, 400))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img = img[0:208, 0:400]\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "tfa.register.register_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./config/hfnet_v2/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Net at 0x7ec5c1b240>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(object):\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.model_filepath = model_path\n",
    "        self.load_graph(model_filepath=self.model_filepath)\n",
    "\n",
    "\n",
    "    def load_graph(self, model_filepath):\n",
    "        print(\"Loading model...\")\n",
    "\n",
    "\n",
    "        with tf.io.gfile.GFile(model_filepath, 'rb') as f:\n",
    "            graph_def = tf.compat.v1.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            with tf.Graph().as_default() as graph:\n",
    "                tf.import_graph_def(graph_def, name='')\n",
    "                signature = tf.compat.v1.saved_model.signature_def_utils.predict_signature_def(\n",
    "                    inputs={'image':graph.get_tensor_by_name('image:0')},\n",
    "                    outputs={'global_descriptor': graph.get_tensor_by_name('global_descriptor:0'),\n",
    "                        'keypoints': graph.get_tensor_by_name('keypoints:0'),\n",
    "                        'local_descriptors': graph.get_tensor_by_name('local_descriptors:0')}\n",
    "                )\n",
    "                builder = tf.compat.v1.saved_model.Builder(\"./config/hfnet_v2/\")\n",
    "                builder.add_meta_graph_and_variables(\n",
    "                    sess=sess,\n",
    "                    tags=[tf.compat.v1.saved_model.tag_constants.SERVING],\n",
    "                    signature_def_map={'serving_default': signature}\n",
    "                )\n",
    "                builder.save()\n",
    "\n",
    "\n",
    "Net(output_frozen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 5735 images\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, (400, 208))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "images = load_images_from_folder(verify_images_path)\n",
    "print(f\"Load {len(images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Linked TensorRT version: (7, 1, 3)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 1, 0)\n",
      "ERROR:tensorflow:Loaded TensorRT 7.1.0 but linked TensorFlow against TensorRT 7.1.3. TensorRT does not support forward compatibility. It is also required to use the same major version of TensorRT during compilation and runtime.\n",
      "INFO:tensorflow:Loaded TensorRT 7.1.0 and linked TensorFlow against TensorRT 7.1.3. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./config/hfnet_v2_trt/assets\n"
     ]
    }
   ],
   "source": [
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS\n",
    "conversion_params = conversion_params._replace(\n",
    "    max_workspace_size_bytes=(1<<30))\n",
    "conversion_params = conversion_params._replace(precision_mode=\"FP16\")\n",
    "conversion_params = conversion_params._replace(\n",
    "    maximum_cached_engines=100)\n",
    "\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "    input_saved_model_dir=model_dir,\n",
    "    conversion_params=conversion_params)\n",
    "converter.convert()\n",
    "\n",
    "def my_input_fn():\n",
    "    for i in range(len(images)):\n",
    "        img = images[i]\n",
    "        _img = np.expand_dims(img, axis=2)\n",
    "        _img = np.array([[_img]]).astype(np.float32)\n",
    "        #print(_img.shape)\n",
    "    yield _img\n",
    "converter.build(input_fn=my_input_fn)\n",
    "converter.save(output_saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Graph 14.587819576263428 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "saved_model_loaded = tf.saved_model.load(\n",
    "    output_saved_model_dir, tags=[tag_constants.SERVING])\n",
    "# saved_model_loaded = tf.saved_model.load(\n",
    "#     output_saved_model_dir)\n",
    "\n",
    "frozen_func = graph_func = saved_model_loaded.signatures[\n",
    "   signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "\n",
    "#frozen_func = convert_to_constants.convert_variables_to_constants_v2(\n",
    "#    graph_func)\n",
    "print(\"--- Loading Graph %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(img):\n",
    "    _img = np.expand_dims(img, axis=2)\n",
    "    _img = np.array([_img]).astype(np.float)\n",
    "    _img = tf.convert_to_tensor(_img, dtype=tf.float32)\n",
    "    #print(_img.shape)\n",
    "    output = frozen_func(_img)\n",
    "    return output\n",
    "\n",
    "img = cv2.imread(\"./db1.jpg\")\n",
    "img = cv2.resize(img, (400, 400))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img = img[0:208, 0:400]\n",
    "print(\"First Try\")\n",
    "start_time = time.time()\n",
    "ret = run(img)\n",
    "print(\"--- First trial %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "def draw(img, ret):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    kps = ret[\"keypoints\"][0].numpy()\n",
    "    for kp in kps:\n",
    "        img = cv2.circle(img, (kp[0], kp[1]), 3, (255, 255, 255))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(0, len(images)):\n",
    "    _time = time.time()\n",
    "    ret = run(images[i])\n",
    "    if (time.time() - _time > 0.01):\n",
    "        print(f\"Img {i} cost {(time.time() - _time)*1000}ms\")\n",
    "        draw(images[i], ret)\n",
    "        \n",
    "print(f\"--- Try {len(images)} images, average {(time.time() - start_time)/len(images)*1000} ms ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[253, 255, 252, ..., 213, 214, 209],\n",
       "       [253, 253, 253, ..., 212, 210, 211],\n",
       "       [251, 252, 251, ..., 212, 211, 211],\n",
       "       ...,\n",
       "       [ 77,  57,  43, ..., 107, 108, 109],\n",
       "       [ 68,  41,  35, ..., 112, 113, 114],\n",
       "       [ 95,  68,  50, ..., 112, 111, 115]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
